{"cells":[{"metadata":{},"cell_type":"markdown","source":"This kernel is most about feature engineering, some in there are good some arent <br />\nAnd there are some simple expainations about what I was doing. <br /> \nBut I think this can be a simple reference to anyone who doesnt have idea how to hit > 0.7 AUC score(Public Score) <br />\nI had to admit, most of my submissions are the blending result of my previous submission.(Since Im kinda lazy, and I just wanted to ran out the submission number) <br />\nBut the improvement was not very effectively. <br />\nThe real improvement was when I did the feature engineering seriouly. <br />\n\n**Here are some place I think it is worth to try/modify/improve this kernel<br />**\n\n1. Missing value filling of Distance\n2. Encoding features with lable ( I had tried few times, it is pretty easy to overfit. But I think it might would be a breaking point )\n3. Feature selection/dropping ( There are many similar features in it, some of them might be bad. But I guess some of the models like gradientBoostingTree,randomForest... should be handle this situation well )\n4. Model tuning/selection ( Might can try stacking or blending, but this will take a lot of time to submit and this is why I was not doing it )\n\nIt is welcome to discuss ! Please feel free to point out my mistake! <br />\nAlso you can download this kernel from my github or from here directly. You can link to my github through my kaggle profile. Thanks, have a nice day."},{"metadata":{},"cell_type":"markdown","source":"# Modules Import"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))\nimport copy\nimport datetime\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.regressor import StackingRegressor\nimport sklearn.metrics\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Checking"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train_offline.csv')\ntest_df = pd.read_csv('../input/test_offline.csv')\n\ntest_ids = test_df[['User_id','Coupon_id','Date_received']]\n\n#Let's Check the shape of data first\nprint(f' training shape : {train_df.shape} ')\nprint(f' testing shape : {test_df.shape} ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def CheckMissingVals(data):\n    for col in data.columns:\n        if np.sum(data[col].isnull()) != 0:\n            print(f' Missing values in {col} : {np.sum(data[col].isnull())}')\n\nprint(\"Count of missing data in training dataset: \")\nCheckMissingVals(train_df)\nprint('\\n')\nprint(\"Count of missing data in testing dataset: \")\nCheckMissingVals(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Deal with missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"total_df = pd.concat([train_df, test_df], axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DistanceFilling_UM = total_df.groupby(['User_id','Merchant_id'])['Distance'].mean().reset_index()\nDistanceFilling_UM.columns = ['User_id','Merchant_id','DistanceFilling_UM']\nDistanceFilling_U = total_df.groupby(['User_id'])['Distance'].mean().reset_index()\nDistanceFilling_U.columns = ['User_id','DistanceFilling_U']\nDistanceFilling_M = total_df.groupby(['Merchant_id'])['Distance'].mean().reset_index()\nDistanceFilling_M.columns = ['Merchant_id','DistanceFilling_M']\n\ntotal_df = pd.merge(total_df,DistanceFilling_UM,on = ['User_id','Merchant_id'], how = 'left')\ntotal_df = pd.merge(total_df,DistanceFilling_U,on = ['User_id'], how = 'left')\ntotal_df = pd.merge(total_df,DistanceFilling_M,on = ['Merchant_id'], how = 'left')\ntotal_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def DistanceMissingFill(data):\n    if np.isnan(data['Distance']):\n        if not np.isnan(data['DistanceFilling_UM']):\n            return int(data['DistanceFilling_UM'])\n        elif not np.isnan(data['DistanceFilling_U']):\n            return int(data['DistanceFilling_U'])\n        elif not  np.isnan(data['DistanceFilling_M']):\n            return int(data['DistanceFilling_M'])\n    return data['Distance']\n\ntotal_df['Distance'] = total_df.apply(DistanceMissingFill, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_df = total_df.drop(['DistanceFilling_UM','DistanceFilling_U','DistanceFilling_M'], axis = 1)\ntrain_df = total_df[:len(train_df)]\ntest_df = total_df[len(train_df):]\n\nprint(\"Count of missing data in training dataset: \")\nCheckMissingVals(train_df)\nprint('\\n')\nprint(\"Count of missing data in testing dataset: \")\nCheckMissingVals(test_df)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Our goal is to predict the use of Coupon, so obviously, if the row doesnt have coupon_id, we dont need to consider such condition.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df[~train_df.Coupon_id.isnull()]\ntest_df = test_df[~test_df.Coupon_id.isnull()]\n\nprint(f' training shape after dropping unwant rows: {train_df.shape} ')\nprint(f' testing shape after dropping unwant rows: {test_df.shape} ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Also we only analysis the use of Coupon within 15 days after user received the Coupon. So If there are some users didnt use the Coupon within 15 days, we consider they didnt use the Coupon, which the probability of using Coupon is zero.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dont forget to drop date column in testing data\ntest_df = test_df.drop(['Date'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fifteenDaysChecking(data):\n    if not np.isnan(data['Date']):\n        #Means the user had used the coupon\n        time_diff = pd.to_datetime(data['Date'], format = \"%Y%m%d\") - pd.to_datetime(data['Date_received'],\n                                                                                   format = \"%Y%m%d\")\n        if time_diff <= pd.Timedelta(15,'D'):\n            return 1\n    return 0\n#This might will take a while\ntrain_df['label'] = train_df.apply(fifteenDaysChecking, axis = 1)\nprint(train_df['label'].value_counts())\ntrain_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.drop(['Date'], axis = 1)\n#Check the missing values again\nprint(\"Count of missing data in training dataset: \")\nCheckMissingVals(train_df)\nprint('\\n')\nprint(\"Count of missing data in testing dataset: \")\nCheckMissingVals(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Merge training & testing dataset for processing\ntrain_label = train_df.label\ntrain_df = train_df.drop(['label'], axis = 1)\ntotal_df = pd.concat([train_df,test_df], axis = 0)\ntotal_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Do a copy of original data , in case we do something wrong for the original data\ntemp_total_df = copy.deepcopy(total_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check the unique counts of each features\ntotal_df.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## date features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert Date_received to str type for datetime processing\ntotal_df['Date_received'] = total_df['Date_received'].astype('int').astype('str')\n#Convert Date_received to other time information\ntotal_df['Date_received'] = total_df['Date_received'].apply(lambda x : datetime.datetime.strptime(x,\"%Y%m%d\"))\ntotal_df['Month_received'] = total_df['Date_received'].apply(lambda x : datetime.datetime.strftime(x,\"%m\")).astype(\"int64\")\ntotal_df['Day_received'] = total_df['Date_received'].apply(lambda x : datetime.datetime.strftime(x,\"%d\")).astype(\"int64\")\n\ntotal_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_df['Month_Cycle'] = total_df.Day_received.map(lambda x : 1 if x <= 15 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Record the total days to June\ndef set2June(data):\n    if data['Month_received'] < 6:\n        return (6-data['Month_received'])*30 - data['Day_received']\n    return 1\n        \ntotal_df['CloseToJune'] = total_df.apply(set2June , axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Discount features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let`s check what values in discount_rate feature\ntotal_df.Discount_rate.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From discount_rate feautre, we can see that there are two types of discount. One of them is directly discount and another is the discount that user's consumption need to reach certain price.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"total_df['DiscountType'] = total_df.Discount_rate.map(lambda x: 1 if (':' in x) else 0)\ntotal_df.DiscountType.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I would like to add threes feature from DiscountType. <br />**\n1. record what price need to reach that allow to get discount <br />\n2. record when the price reach to discount, how much money can be discount <br />\n3. record the discount ratio of both discount type <br />\n4. record how much money we need to cost after reach the distcount bound"},{"metadata":{"trusted":true},"cell_type":"code","source":"total_df['DiscountBound'] = total_df.Discount_rate.map(lambda x: int(x.split(':')[0]) if (':' in x) else 0)\ntotal_df['DirectPriceCut'] = total_df.Discount_rate.map(lambda x: int(x.split(':')[1]) if (':' in x) else 0)\ntotal_df['DiscountRatio'] = total_df.Discount_rate.map(lambda x: (1 - float(x.split(':')[1])/float(x.split(':')[0])) if(':' in x) else float(x) )\ntotal_df['MoneyCost'] = total_df['DiscountBound'] - total_df['DirectPriceCut']\ntotal_df = total_df.drop(['Discount_rate'], axis = 1)\ntotal_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I would like to group the Discount upper bound**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check the data distribution fitst\ntotal_df['DiscountBound'].hist()\nplt.title('Histogram of Discount Bound')\nplt.xlabel('DiscountBound')\nplt.ylabel('Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cuttingArr = np.array([-1,80,150,250,total_df.DiscountBound.max()+1])\ntotal_df['DiscountBound_Group'] = pd.cut(total_df.DiscountBound, cuttingArr)\n\nprint(total_df.DiscountBound_Group.value_counts())\nprint('\\n')\n#encoding intervals into integer\nDiscountBound_intervals = total_df.DiscountBound_Group.unique()\ntotal_df.DiscountBound_Group.replace(to_replace = DiscountBound_intervals[0],value = 0, inplace = True)\ntotal_df.DiscountBound_Group.replace(to_replace = DiscountBound_intervals[1],value = 1, inplace = True)\ntotal_df.DiscountBound_Group.replace(to_replace = DiscountBound_intervals[2],value = 2, inplace = True)\ntotal_df.DiscountBound_Group.replace(to_replace = DiscountBound_intervals[3],value = 3, inplace = True)\nprint(total_df.DiscountBound_Group.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## User id features"},{"metadata":{},"cell_type":"markdown","source":"**I would like to use User_id feature to group with other features. I hope this can find some characteristic or behavior of user**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Same_Merchant_User_received = total_df[['User_id','Merchant_id']]\nSame_Merchant_User_received['temp'] = 1\nSame_Merchant_User_received = Same_Merchant_User_received.groupby(['User_id','Merchant_id']).agg('sum').reset_index()\nSame_Merchant_User_received.columns = ['User_id','Merchant_id','Same_Merchant_User_received']\n\ntotal_df = pd.merge(total_df, Same_Merchant_User_received, on = ['User_id','Merchant_id'], how = 'left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Record how many coupons each user received**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nTotal_Coupon_User_received = total_df.groupby(['User_id'])['Coupon_id'].count().reset_index()\nTotal_Coupon_User_received.columns = ['User_id', 'Total_Coupon_User_received']\ntotal_df = pd.merge(total_df,Total_Coupon_User_received, on = ['User_id'], how = 'left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Record how many same coupons each user received**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Same_Coupon_User_received = total_df[['User_id', 'Coupon_id']]\nSame_Coupon_User_received['temp'] = 1\nSame_Coupon_User_received = Same_Coupon_User_received.groupby(['User_id','Coupon_id']).agg('sum').reset_index()\nSame_Coupon_User_received.columns = ['User_id','Coupon_id','Same_Coupon_User_received']\n\ntotal_df = pd.merge(total_df,Same_Coupon_User_received , on = ['User_id', 'Coupon_id'], how = 'left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Record how many same coupons each user received at the same day**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Same_Day_Same_Coupon_User_received = total_df[['User_id','Coupon_id','Date_received']]\nSame_Day_Same_Coupon_User_received['temp'] = 1\nSame_Day_Same_Coupon_User_received = Same_Day_Same_Coupon_User_received.groupby(['User_id','Coupon_id','Date_received']).agg('sum').reset_index()\nSame_Day_Same_Coupon_User_received.columns = ['User_id','Coupon_id','Date_received','Same_Day_Same_Coupon_User_received']\n\ntotal_df = pd.merge(total_df, Same_Day_Same_Coupon_User_received , on = ['User_id','Coupon_id','Date_received'], how = 'left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Record how many coupons each user received at the same day**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nSame_Day_Total_Coupon_User_received = total_df[['User_id','Date_received']]\nSame_Day_Total_Coupon_User_received['temp'] = 1\nSame_Day_Total_Coupon_User_received = Same_Day_Total_Coupon_User_received.groupby(['User_id','Date_received']).agg('sum').reset_index()\nSame_Day_Total_Coupon_User_received.columns = ['User_id','Date_received', 'Same_Day_Total_Coupon_User_received']\n\ntotal_df = pd.merge(total_df, Same_Day_Total_Coupon_User_received, on = ['User_id', 'Date_received'], how ='left')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Record the mean/max/min discount upper bound of coupon that each user received**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nUser_Received_Coupon_DistcountBound_Mean = total_df.groupby(['User_id'])['DiscountBound'].mean().reset_index()\nUser_Received_Coupon_DistcountBound_Mean.columns = ['User_id','User_Received_Coupon_DistcountBound_Mean']\n\nUser_Received_Coupon_DistcountBound_Max = total_df.groupby(['User_id'])['DiscountBound'].max().reset_index()\nUser_Received_Coupon_DistcountBound_Max.columns = ['User_id','User_Received_Coupon_DistcountBound_Max']\n\nUser_Received_Coupon_DistcountBound_Min = total_df.groupby(['User_id'])['DiscountBound'].min().reset_index()\nUser_Received_Coupon_DistcountBound_Min.columns = ['User_id','User_Received_Coupon_DistcountBound_Min']\n\ntotal_df = pd.merge(total_df,User_Received_Coupon_DistcountBound_Mean, on = ['User_id'], how = 'left')\ntotal_df = pd.merge(total_df,User_Received_Coupon_DistcountBound_Max, on = ['User_id'], how = 'left')\ntotal_df = pd.merge(total_df,User_Received_Coupon_DistcountBound_Min, on = ['User_id'], how = 'left')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Record the mean/max/min Money Cost of coupon that each user received**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nUser_Received_Coupon_MoneyCost_Mean = total_df.groupby(['User_id'])['MoneyCost'].mean().reset_index()\nUser_Received_Coupon_MoneyCost_Mean.columns = ['User_id','User_Received_Coupon_MoneyCost_Mean']\n\nUser_Received_Coupon_MoneyCost_Max = total_df.groupby(['User_id'])['MoneyCost'].max().reset_index()\nUser_Received_Coupon_MoneyCost_Max.columns = ['User_id','User_Received_Coupon_MoneyCost_Max']\n\nUser_Received_Coupon_MoneyCost_Min = total_df.groupby(['User_id'])['MoneyCost'].min().reset_index()\nUser_Received_Coupon_MoneyCost_Min.columns = ['User_id','User_Received_Coupon_MoneyCost_Min']\n\ntotal_df = pd.merge(total_df,User_Received_Coupon_MoneyCost_Mean, on = ['User_id'], how = 'left')\ntotal_df = pd.merge(total_df,User_Received_Coupon_MoneyCost_Max, on = ['User_id'], how = 'left')\ntotal_df = pd.merge(total_df,User_Received_Coupon_MoneyCost_Min, on = ['User_id'], how = 'left')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Record the mean discount ratio of coupon that each user received**"},{"metadata":{"trusted":true},"cell_type":"code","source":"User_Received_Coupon_DiscountRatio_Mean = total_df.groupby(['User_id'])['DiscountRatio'].mean().reset_index()\nUser_Received_Coupon_DiscountRatio_Mean.columns = ['User_id','User_Received_Coupon_DiscountRatio_Mean']\ntotal_df = pd.merge(total_df,User_Received_Coupon_DiscountRatio_Mean, on = ['User_id'], how = 'left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Record the minimal days that user received the same coupon. If the user only got the coupon once, then make the day gaps big.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#This will take a long~ time\ndef CalMinReceivedGap(data):\n\n    dates = data.split(':')\n    if len(dates) == 1:\n        return -1 #Means only received once\n    MinGap = 365\n    for i in range(0,len(dates)-1):\n        cur_gap = pd.to_datetime( dates[i] , format = \"%Y-%m-%d\") - pd.to_datetime(dates[i+1] , format = \"%Y-%m-%d\")\n        cur_gap = abs(cur_gap.days)\n        if cur_gap < MinGap:\n                MinGap = cur_gap\n    return MinGap\n\nMin_Coupon_received_gap = total_df[['User_id','Coupon_id','Date_received']]\nMin_Coupon_received_gap.Date_received = Min_Coupon_received_gap.Date_received.astype('str')\nMin_Coupon_received_gap =  Min_Coupon_received_gap.groupby(['User_id','Coupon_id'])['Date_received'].agg(lambda x : ':'.join(x)).reset_index()\nMin_Coupon_received_gap['Min_Coupon_received_gap'] = Min_Coupon_received_gap.Date_received.apply(CalMinReceivedGap)\nMin_Coupon_received_gap = Min_Coupon_received_gap.drop(['Date_received'], axis = 1)\n\ntotal_df = pd.merge(total_df,Min_Coupon_received_gap,on = ['User_id','Coupon_id'], how ='left')\nmax_received_gap = total_df.Min_Coupon_received_gap.max()\ntotal_df.Min_Coupon_received_gap = total_df.Min_Coupon_received_gap.map(lambda x : x if x != -1 else max_received_gap)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Record the maximum days that user received the same coupon. If the user only got the coupon once, then make the day gaps big.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def CalMaxReceivedGap(data):\n    dates = data.split(':')\n    if len(dates) == 1:\n        return -1 #Means only received once\n    MaxGap = 0\n    for i in range(0,len(dates)-1):\n        cur_gap = pd.to_datetime( dates[i] , format = \"%Y-%m-%d\") - pd.to_datetime(dates[i+1] , format = \"%Y-%m-%d\")\n        cur_gap = abs(cur_gap.days)\n        if cur_gap > MaxGap:\n                MaxGap = cur_gap\n    return MaxGap\n\n\nMax_Coupon_received_gap = total_df[['User_id','Coupon_id','Date_received']]\nMax_Coupon_received_gap.Date_received = Max_Coupon_received_gap.Date_received.astype('str')\nMax_Coupon_received_gap =  Max_Coupon_received_gap.groupby(['User_id','Coupon_id'])['Date_received'].agg(lambda x : ':'.join(x)).reset_index()\nMax_Coupon_received_gap['Max_Coupon_received_gap'] = Max_Coupon_received_gap.Date_received.apply(CalMaxReceivedGap)\nMax_Coupon_received_gap = Max_Coupon_received_gap.drop(['Date_received'], axis = 1)\n\ntotal_df = pd.merge(total_df,Max_Coupon_received_gap,on = ['User_id','Coupon_id'], how ='left')\nmax_received_gap = total_df.Max_Coupon_received_gap.max()\ntotal_df.Max_Coupon_received_gap = total_df.Max_Coupon_received_gap.map(lambda x : x if x != -1 else max_received_gap)\ntotal_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Merchant features"},{"metadata":{},"cell_type":"markdown","source":"**Here I will use Merchant_id feature to group with other features. This just vey similar to the processing of User_id feature.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nMerchant_DiscountBound_mean = total_df.groupby(['Merchant_id'])['DiscountBound'].mean().reset_index()\nMerchant_DiscountBound_mean.columns = ['Merchant_id','Merchant_DiscountBound_mean']\n\nMerchant_DiscountBound_max = total_df.groupby(['Merchant_id'])['DiscountBound'].max().reset_index()\nMerchant_DiscountBound_max.columns = ['Merchant_id','Merchant_DiscountBound_max']\n\nMerchant_DiscountBound_min = total_df.groupby(['Merchant_id'])['DiscountBound'].min().reset_index()\nMerchant_DiscountBound_min.columns = ['Merchant_id','Merchant_DiscountBound_min']\n\nMerchant_DiscountRatio_max = total_df.groupby(['Merchant_id'])['DiscountRatio'].max().reset_index()\nMerchant_DiscountRatio_max.columns = ['Merchant_id','Merchant_DiscountRatio_max']\n\nMerchant_DiscountRatio_min = total_df.groupby(['Merchant_id'])['DiscountRatio'].min().reset_index()\nMerchant_DiscountRatio_min.columns = ['Merchant_id','Merchant_DiscountRatio_min']\n\nMerchant_DirectPriceCut_max = total_df.groupby(['Merchant_id'])['DirectPriceCut'].max().reset_index()\nMerchant_DirectPriceCut_max.columns = ['Merchant_id','Merchant_DirectPriceCut_max']\n\nMerchant_DirectPriceCut_min = total_df.groupby(['Merchant_id'])['DirectPriceCut'].min().reset_index()\nMerchant_DirectPriceCut_min.columns = ['Merchant_id','Merchant_DirectPriceCut_min']\n\nMerchant_DirectPriceCut_mean = total_df.groupby(['Merchant_id'])['DirectPriceCut'].mean().reset_index()\nMerchant_DirectPriceCut_mean.columns = ['Merchant_id', 'Merchant_DirectPriceCut_mean']\n\nMerchant_MoneyCost_mean = total_df.groupby(['Merchant_id'])['MoneyCost'].mean().reset_index()\nMerchant_MoneyCost_mean.columns = ['Merchant_id', 'Merchant_MoneyCost_mean']\n\nMerchant_MoneyCost_max = total_df.groupby(['Merchant_id'])['MoneyCost'].max().reset_index()\nMerchant_MoneyCost_max.columns = ['Merchant_id', 'Merchant_MoneyCost_max']\n\nMerchant_MoneyCost_min = total_df.groupby(['Merchant_id'])['MoneyCost'].min().reset_index()\nMerchant_MoneyCost_min.columns = ['Merchant_id', 'Merchant_MoneyCost_min']\n\ntotal_df = pd.merge(total_df,Merchant_DiscountBound_mean,on = ['Merchant_id'], how = 'left')\ntotal_df = pd.merge(total_df,Merchant_DirectPriceCut_mean,on = ['Merchant_id'], how = 'left')\ntotal_df = pd.merge(total_df,Merchant_MoneyCost_mean,on = ['Merchant_id'], how = 'left')\ntotal_df = pd.merge(total_df,Merchant_DiscountBound_max, on = ['Merchant_id'], how = 'left')\ntotal_df = pd.merge(total_df,Merchant_DiscountBound_min, on = ['Merchant_id'], how = 'left')\ntotal_df = pd.merge(total_df,Merchant_DiscountRatio_max, on = ['Merchant_id'], how = 'left')\ntotal_df = pd.merge(total_df,Merchant_DiscountRatio_min, on = ['Merchant_id'], how = 'left')\ntotal_df = pd.merge(total_df,Merchant_DirectPriceCut_max, on = ['Merchant_id'], how = 'left')\ntotal_df = pd.merge(total_df,Merchant_DirectPriceCut_min, on = ['Merchant_id'], how = 'left')\ntotal_df = pd.merge(total_df,Merchant_MoneyCost_max, on = ['Merchant_id'], how = 'left')\ntotal_df = pd.merge(total_df,Merchant_MoneyCost_min, on = ['Merchant_id'], how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Mcount = total_df.Merchant_id.value_counts().reset_index()\nMcount.columns = ['Merchant_id','Merchant_count']\nCcount = total_df.Coupon_id.value_counts().reset_index()\nCcount.columns = ['Coupon_id','Coupon_count']\n\ntotal_df = pd.merge(total_df,Mcount,on = ['Merchant_id'], how = 'left')\ntotal_df = pd.merge(total_df,Ccount,on = ['Coupon_id'], how = 'left')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let check the correlation between label and features to judge what we just did is good or not**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check the correlation again before fill the missing values of Distance\ncorrelation = copy.deepcopy(total_df[:len(train_df)])\ncorrelation = pd.concat([correlation, pd.DataFrame(train_label.values, columns = ['label'], index = correlation.index)], axis = 1)\ncorr = correlation.corr()\nplt.figure(figsize = (30,30))\nsns.heatmap(corr, cmap = plt.cm.summer, annot = True)\nplt.title('Correlation of training dataset')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distance features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#There are still some Distance values missing\n#Let's check the ratio between missing values and non-missing values\n\nprint(f' Missing ratio of Distance : {np.sum(total_df.Distance.isnull()) / len(total_df)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I have a bold idea, the distance might will have little correlation of when user received the coupon. Since the merchant might would send the coupon in certain days. So If there are some users received the coupon at similar time, they might had got the coupon at the same merchant. Which means it has slightly chance that these users have similar event location. "},{"metadata":{"trusted":true},"cell_type":"code","source":"total_df['Distance'] = total_df.groupby(['CloseToJune'])['Distance'].transform(lambda x : x.fillna(x.mode()[0]))\nCheckMissingVals(total_df)\ntotal_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"User_Activity_Distance_Mean = total_df.groupby(['User_id'])['Distance'].mean().reset_index()\nUser_Activity_Distance_Mean.columns = ['User_id','User_Activity_Distance_Mean']\ntotal_df = pd.merge(total_df,User_Activity_Distance_Mean,on = ['User_id'], how = 'left')\n\nUser_Activity_Distance_Max = total_df.groupby(['User_id'])['Distance'].max().reset_index()\nUser_Activity_Distance_Max.columns = ['User_id','User_Activity_Distance_Max']\ntotal_df = pd.merge(total_df,User_Activity_Distance_Max,on = ['User_id'], how = 'left')\n\nUser_Activity_Distance_Min = total_df.groupby(['User_id'])['Distance'].min().reset_index()\nUser_Activity_Distance_Min.columns = ['User_id','User_Activity_Distance_Min']\ntotal_df = pd.merge(total_df,User_Activity_Distance_Min,on = ['User_id'], how = 'left')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_df['Bound_Distance'] = total_df['DiscountBound']+1 * (total_df['Distance']+1)\ntotal_df['DiscountRatio_Distance'] = total_df['DiscountRatio'] * (total_df['Distance']+1)\ntotal_df['MoneyCost_Distance'] = total_df['MoneyCost']+1 * (total_df['Distance']+1)\ntotal_df['DirectPriceCut_Distance'] = total_df['DirectPriceCut']+1 / (total_df['Distance']+1) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Merchant_Distance_mean = total_df.groupby(['Merchant_id'])['Distance'].mean().reset_index()\nMerchant_Distance_mean.columns = ['Merchant_id','Merchant_Distance_mean']\n\nMerchant_Distance_max = total_df.groupby(['Merchant_id'])['Distance'].max().reset_index()\nMerchant_Distance_max.columns = ['Merchant_id','Merchant_Distance_max']\n\nMerchant_Distance_min = total_df.groupby(['Merchant_id'])['Distance'].min().reset_index()\nMerchant_Distance_min.columns = ['Merchant_id','Merchant_Distance_min']\n\n\nCoupon_Distance_mean = total_df.groupby(['Coupon_id'])['Distance'].mean().reset_index()\nCoupon_Distance_mean.columns = ['Coupon_id', 'Coupon_Distance_mean']\n\nCoupon_Distance_max = total_df.groupby(['Coupon_id'])['Distance'].max().reset_index()\nCoupon_Distance_max.columns = ['Coupon_id', 'Coupon_Distance_max']\n\nCoupon_Distance_min = total_df.groupby(['Coupon_id'])['Distance'].min().reset_index()\nCoupon_Distance_min.columns = ['Coupon_id', 'Coupon_Distance_min']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_df = pd.merge(total_df,Merchant_Distance_mean,on = ['Merchant_id'], how = 'left')\ntotal_df = pd.merge(total_df,Merchant_Distance_max,on = ['Merchant_id'], how = 'left')\ntotal_df = pd.merge(total_df,Merchant_Distance_min,on = ['Merchant_id'], how = 'left')\n\ntotal_df = pd.merge(total_df,Coupon_Distance_mean, on = ['Coupon_id'], how = 'left')\ntotal_df = pd.merge(total_df,Coupon_Distance_max, on = ['Coupon_id'], how = 'left')\ntotal_df = pd.merge(total_df,Coupon_Distance_min, on = ['Coupon_id'], how = 'left')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features encoding with label"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ntrainSet = copy.deepcopy(total_df[:len(train_df)])\ntrainSet = pd.concat([trainSet, pd.DataFrame(train_label.values, columns = ['label'], index = trainSet.index)], axis = 1)\ntrainSet.head(3)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nDistance_Accepted_rate = trainSet.groupby(['Distance'])['label'].mean().reset_index()\nDistance_Accepted_rate.columns = ['Distance','Distance_Accepted_rate']\n\nDiscountBound_Accepted_rate = trainSet.groupby(['DiscountBound'])['label'].mean().reset_index()\nDiscountBound_Accepted_rate.columns = ['DiscountBound', 'DiscountBound_Accepted_rate']\n\ntotal_df = pd.merge(total_df, Distance_Accepted_rate , on = ['Distance'], how = 'left')\ntotal_df = pd.merge(total_df, DiscountBound_Accepted_rate, on = ['DiscountBound'], how = 'left')\n\ntotal_df.head(5)\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scale the features"},{"metadata":{"trusted":true},"cell_type":"code","source":"total_df = total_df.drop(['Merchant_id','Coupon_id','Day_received','Month_received','User_id','Date_received'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor col in total_df.columns:\n    if (col != 'Distance_Accepted_rate') & (col != 'DiscountBound_Accepted_rate'):\n        total_df[col] = MinMaxScaler().fit_transform(total_df[col].values.reshape(-1,1))        \ntotal_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#One hot encoding the discount type\ntotal_df = pd.get_dummies(total_df, columns = ['DiscountType'], prefix = \"DiscountType\")\nprint(f' final shape of total data : {total_df.shape} ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check the correlation again before fill the missing values of Distance\ncorrelation = copy.deepcopy(total_df[:len(train_df)])\ncorrelation = pd.concat([correlation, pd.DataFrame(train_label.values, columns = ['label'], index = correlation.index)], axis = 1)\ncorr = correlation.corr()\nplt.figure(figsize = (30,30))\nsns.heatmap(corr, cmap = plt.cm.summer, annot = True)\nplt.title('Correlation of training dataset')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model training & validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = total_df[:len(train_df)]\ntest_x = total_df[len(train_df):]\n\ndef tuneParamsRandom(classifier, params, train_x, train_y, cv = 5):\n    rs = RandomizedSearchCV(classifier, params, n_iter = 20, scoring = 'roc_auc', n_jobs = -1, verbose = 0, cv=cv)\n    rs.fit(train_x, train_y)\n    \n    return rs.best_params_, abs(rs.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\netParams = {'n_estimators':np.arange(100,1100,50), 'max_depth':np.arange(3,11,2),\n            'min_samples_leaf':np.arange(1,6,2) }\n\nrfParams = {'n_estimators':np.arange(100,1100,50), 'max_depth':np.arange(3,11,2),\n            'min_samples_split':np.arange(2,20,2), 'min_samples_leaf':np.arange(1,6,2) }\n\nabParams = {'n_estimators':np.arange(100,1100,50),'learning_rate':np.arange(0.01,0.2,0.05)}\n\n\nlgbmcParams = {'n_estimators' : np.arange(400,1200,100), 'learning_rate' : np.arange(0.01,0.1,0.02),\n               'num_leaves' : np.arange(2,48,4), 'max_depth' : np.arange(3,10,2),\n               'subsample' : np.arange(0.3,0.8,0.1) }\n\ngbcParams = {'n_estimators' : np.arange(400,1200,100) , 'learning_rate' : np.arange(0.01,0.1,0.02) \n             , 'min_samples_split' : np.arange(2,30,5), 'min_samples_leaf' : np.arange(2,32,4),\n              'max_depth' : np.arange(3,8,2), 'subsample' : np.arange(0.3,0.8,0.1)}\n\nxgbParams = {'max_depth':np.arange(3,8,1),'learning_rate':np.arange(0.01,0.1,0.02)\n             ,'n_estimator': np.arange(1000,3000,100),'gamma':np.arange(0.01,0.1,0.02)}\n\nlrParams = {'C':np.arange(0.01,1,0.05), 'max_iter' : np.arange(100,500,100)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_label.index = train_x.index\ntuneSet = pd.concat([train_x,train_label], axis = 1)\ntrainSet = tuneSet.sample(frac=0.5)\ntrainSet.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(trainSet[trainSet.columns[trainSet.columns != 'label']],trainSet['label'], test_size = 0.3, random_state = 1234)\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LogisticRegression"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nlr = LogisticRegression(random_state = 2019,n_jobs = -1)\nlr_best_Params, lr_best_score = tuneParamsRandom(lr,lrParams,x_train,y_train)\nprint(\"LogisticRegression :\",lr_best_Params,lr_best_score)\nlr = LogisticRegression(**lr_best_Params,n_jobs = -1)\nlr.fit(x_train,y_train)\nlr_pred_val = lr.predict_proba(x_test)\nauc_score = roc_auc_score(y_true = y_test, y_score = lr_pred_val[:,1])\nacc = accuracy_score(y_true = y_test, y_pred = lr_pred_val.argmax(axis=1))\nprint(\"Validation AUC: {:.3f}, Accuracy: {:.3f}\".format(auc_score, acc))\nlr = LogisticRegression(**lr_best_Params,n_jobs = -1,random_state = 2019)\nlr.fit(train_x, train_label)\nlr_pred = lr.predict_proba(test_x)[:,1]\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GradientBoostingClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ngbc = GradientBoostingClassifier(max_features = 'sqrt')\ngbc_best_Params,gbc_best_score = tuneParamsRandom(gbc,gbcParams,x_train,y_train)\nprint(\"GradientBoostingClassifier:\",gbc_best_Params,gbc_best_score)\n\n\ngbc = GradientBoostingClassifier(**gbc_best_Params,max_features = 'sqrt')\ngbc.fit(train_x, train_label)\ngbc_pred = gbc.predict_proba(test_x)[:,1]\n\nimportances = pd.DataFrame(gbc.feature_importances_, columns = ['importances'], index = train_x.columns)\nimportances.importances.sort_values(ascending = False)\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LGBMClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlgbmc_best_Params, lgbmc_best_score = tuneParamsRandom(LGBMClassifier(),lgbmcParams,x_train,y_train)\nprint(\"LGBMClassifier:\",lgbmc_best_Params,lgbmc_best_score)\n\n\nlgbmc = LGBMClassifier(**lgbmc_best_Params)\nlgbmc.fit(train_x,train_label)\nlgbmc_pred = lgbmc.predict_proba(test_x)[:,1]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nxgb_best_Params,xgb_best_score = tuneParamsRandom(XGBClassifier(),xgbParams, x_train, y_train)\nprint(\"XGBClassifier:\",xgb_best_Params,xgb_best_score)\n\nxgb = XGBClassifier(**xgb_best_Params)\nxgb.fit(train_x,train_label)\nxgb_pred = xgb.predict_proba(test_x)[:,1]\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## AdaBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nab_best_Params,ab_best_score = tuneParamsRandom(AdaBoostClassifier(),abParams,val_x,val_y)\nprint(\"AdaBoost:\",ab_best_Params,ab_best_score)\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RandomForest"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nrf_best_Params,rf_best_score = tuneParamsRandom(RandomForestClassifier(),rfParams,val_x,val_y)\nprint(\"RandomForest:\",rf_best_Params,rf_best_score)\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Blending"},{"metadata":{"trusted":true},"cell_type":"code","source":"#blending_pred = gbc_pred * 0.3 + lgbmc_pred * 0.7 ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stacking"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nfrom mlxtend.classifier import StackingClassifier\ngbc = GradientBoostingClassifier(**gbc_best_Params)\nlgbmc = LGBMClassifier(**lgbmc_best_Params)\nxgb = XGBClassifier(**xgb_best_Params)\n\nmeta_estimator = LGBMClassifier()\nstacking = StackingClassifier(classifiers = [gbc,lgbmc, xgb], meta_classifier = meta_estimator)\nstacking.fit(train_x,train_label)\nstacking_pred = stacking.predict_proba(test_x)[:,1]\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ids = test_ids.loc[~test_ids.Coupon_id.isna()]\n\ntest_ids = pd.concat([test_ids, pd.DataFrame(lgbmc_pred, columns = ['label'], index = test_ids.index)] , axis = 1)\n\ntest_ids.loc[:,\"User_id\"] = test_ids[\"User_id\"].apply(lambda x:str(int(x)))\ntest_ids.loc[:,\"Coupon_id\"] = test_ids[\"Coupon_id\"].apply(lambda x:str(int(x)))\ntest_ids.loc[:,\"Date_received\"] = test_ids[\"Date_received\"].apply(lambda x: str(int(x)))\n\ntest_ids[\"uid\"] = test_ids[['User_id',\"Coupon_id\",\"Date_received\"]].apply(lambda x: '_'.join(x.values), axis = 1)\ntest_ids.reset_index(drop = True, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = test_ids.groupby(\"uid\", as_index = False).mean()\nsubmission = submission[[\"uid\",\"label\"]]\nsubmission.to_csv(\"v9.csv\", header = [\"uid\", \"label\"], index = False)\n\nsubmission.head(10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}