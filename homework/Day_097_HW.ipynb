{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "_KWWOQhgxOd2",
    "outputId": "6811eaaf-ba49-405d-82d6-053cc618d5ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug  2 05:48:37 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   61C    P0    28W /  70W |   2387MiB / 15079MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "import os\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "x2D7rszlzZEv",
    "outputId": "ad7b808e-d456-41fc-805d-d02f2e8050b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x_train shape : (50000, 3072)\n",
      " train_y shape : (50000, 10)\n",
      " x_test shape : (10000, 3072)\n",
      " test_y shape : (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "(train_x, train_y) , (test_x, test_y) = cifar10.load_data()\n",
    "\n",
    "def preproc_x(x, flatten = True):\n",
    "  x = x / 255.0\n",
    "  if flatten:\n",
    "    x = x.reshape((len(x),-1))\n",
    "  return x\n",
    "\n",
    "def preproc_y(y, num_classes = 10):\n",
    "  if y.shape[-1] == 1:\n",
    "    y = keras.utils.to_categorical(y,num_classes)\n",
    "    \n",
    "  return y\n",
    "\n",
    "\n",
    "x_train = preproc_x(train_x)\n",
    "x_test = preproc_x(test_x)\n",
    "train_y = preproc_y(train_y)\n",
    "test_y = preproc_y(test_y)\n",
    "\n",
    "\n",
    "print(f' x_train shape : {x_train.shape}')\n",
    "print(f' train_y shape : {train_y.shape}')\n",
    "print(f' x_test shape : {x_test.shape}')\n",
    "print(f' test_y shape : {test_y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VJHzstvp0UMD"
   },
   "outputs": [],
   "source": [
    "def build_MLP(input_shape, output_units = 10):\n",
    "  \n",
    "  input_layer = keras.layers.Input(input_shape)\n",
    "  x = Dense(units = 512, activation = 'relu')(input_layer)\n",
    "  x = Dropout(0.2)(x)\n",
    "  x = Dense(units = 512, activation = 'relu')(x)\n",
    "  x = Dropout(0.2)(x)\n",
    "  output_layer = Dense(units = output_units, activation = 'softmax')(x)\n",
    "  \n",
    "  model = keras.models.Model( inputs = [input_layer], outputs = [output_layer])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZvJ5hfFi1NpT"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 742
    },
    "colab_type": "code",
    "id": "EJNf4tIN0MU6",
    "outputId": "b0b0a001-7e6b-40a1-cd38-95ae0c3ecfe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               1573376   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,841,162\n",
      "Trainable params: 1,841,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 3s 60us/step - loss: 2.2541 - acc: 0.2466 - val_loss: 1.8275 - val_acc: 0.3319\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 3s 53us/step - loss: 1.8545 - acc: 0.3299 - val_loss: 1.7505 - val_acc: 0.3682\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 3s 53us/step - loss: 1.7770 - acc: 0.3612 - val_loss: 1.7104 - val_acc: 0.3947\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 3s 53us/step - loss: 1.7344 - acc: 0.3801 - val_loss: 1.6711 - val_acc: 0.4169\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 3s 53us/step - loss: 1.6971 - acc: 0.3891 - val_loss: 1.6563 - val_acc: 0.4054\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 3s 53us/step - loss: 1.6724 - acc: 0.4014 - val_loss: 1.6511 - val_acc: 0.4092\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 3s 53us/step - loss: 1.6508 - acc: 0.4093 - val_loss: 1.5790 - val_acc: 0.4450\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 3s 52us/step - loss: 1.6371 - acc: 0.4137 - val_loss: 1.6327 - val_acc: 0.4207\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 3s 53us/step - loss: 1.6202 - acc: 0.4192 - val_loss: 1.5834 - val_acc: 0.4338\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 3s 53us/step - loss: 1.6120 - acc: 0.4257 - val_loss: 1.5792 - val_acc: 0.4519\n",
      " Test loss : 1.579194370651245\n",
      " Test accuracy : 0.4519\n"
     ]
    }
   ],
   "source": [
    "mlp = build_MLP(x_train.shape[1:])\n",
    "mlp.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'], optimizer = RMSprop())\n",
    "\n",
    "mlp.summary()\n",
    "\n",
    "history = mlp.fit(x_train, train_y, batch_size = batch_size, epochs = epochs, shuffle = True, validation_data = (x_test, test_y), verbose = 1)\n",
    "score = mlp.evaluate(x_test, test_y, verbose = 0)\n",
    "print(f' Test loss : {score[0]}')\n",
    "print(f' Test accuracy : {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "6Izcepy310nH",
    "outputId": "eb441bc6-809f-4cf7-8aca-24d2f81cd9e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train_x shape : (50000, 3072)\n",
      " train_y shape : (50000, 10)\n",
      " test_x shape : (10000, 32, 32, 3)\n",
      " test_y shape : (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "train_x = preproc_x(train_x, False)\n",
    "test_x = preproc_x(test_x, False)\n",
    "\n",
    "print(f' train_x shape : {x_train.shape}')\n",
    "print(f' train_y shape : {train_y.shape}')\n",
    "print(f' test_x shape : {test_x.shape}')\n",
    "print(f' test_y shape : {test_y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_c1AxTUQGvGs"
   },
   "outputs": [],
   "source": [
    "def build_CNN(input_shape, output_units = 10):\n",
    "  \n",
    "  \n",
    "  input_layer = keras.layers.Input(input_shape)\n",
    "  \n",
    "  x = Conv2D(filters = 64, kernel_size = (3,3), padding = 'same')(input_layer)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = Conv2D(filters = 64, kernel_size = (3,3), padding = 'same')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = MaxPooling2D(pool_size = (2,2))(x)\n",
    "  x = Dropout(0.25)(x)\n",
    "  x = Conv2D(filters = 32, kernel_size = (3,3), padding = 'same')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = Conv2D(filters = 32, kernel_size = (3,3), padding = 'same')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = MaxPooling2D(pool_size = (2,2))(x)\n",
    "  x = Dropout(0.25)(x)\n",
    "  x = Flatten()(x)\n",
    "  x = Dense(units = 512)(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = Dropout(0.25)(x)\n",
    "  x = Dense(units = 256)(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = Dropout(0.25)(x)\n",
    "  x = Dense(units = 128)(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = Dropout(0.25)(x)\n",
    "  \n",
    "  output_layer = Dense(units = output_units, activation = 'softmax')(x)\n",
    "  \n",
    "  model = keras.models.Model( inputs = [input_layer], outputs = [output_layer])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_2Uu7srqJM0j"
   },
   "outputs": [],
   "source": [
    "dataGenerator = ImageDataGenerator( rotation_range = 10,\n",
    "                                    width_shift_range = 0.1,\n",
    "                                    height_shift_range = 0.1,\n",
    "                                    zoom_range = 0.1 )\n",
    "\n",
    "\n",
    "dataGenerator.fit(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Wz1z9gd1Jo6L",
    "outputId": "307655e4-3802-482e-85eb-49c5a253a442"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 16, 16, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,285,386\n",
      "Trainable params: 1,283,210\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model = build_CNN(train_x.shape[1:])\n",
    "sgd = keras.optimizers.SGD(lr = 0.001, momentum = 0.95, nesterov = True)\n",
    "adam = keras.optimizers.Adam(lr = 0.001)\n",
    "cnn_model.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'], optimizer = adam)\n",
    "cnn_model.summary()\n",
    "\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, min_lr = 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4oAFxT2gT3jB",
    "outputId": "b49b578c-8744-471d-d6a6-1a843eb12554"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "391/390 [==============================] - 29s 74ms/step - loss: 1.6457 - acc: 0.4042 - val_loss: 1.8936 - val_acc: 0.3573\n",
      "Epoch 2/100\n",
      "391/390 [==============================] - 26s 66ms/step - loss: 1.2687 - acc: 0.5422 - val_loss: 1.3630 - val_acc: 0.5240\n",
      "Epoch 3/100\n",
      "391/390 [==============================] - 25s 65ms/step - loss: 1.0906 - acc: 0.6102 - val_loss: 1.2351 - val_acc: 0.5727\n",
      "Epoch 4/100\n",
      "391/390 [==============================] - 26s 65ms/step - loss: 0.9842 - acc: 0.6538 - val_loss: 1.1517 - val_acc: 0.5912\n",
      "Epoch 5/100\n",
      "391/390 [==============================] - 25s 64ms/step - loss: 0.9164 - acc: 0.6779 - val_loss: 0.9507 - val_acc: 0.6684\n",
      "Epoch 6/100\n",
      "391/390 [==============================] - 25s 64ms/step - loss: 0.8605 - acc: 0.6996 - val_loss: 0.7849 - val_acc: 0.7348\n",
      "Epoch 7/100\n",
      "391/390 [==============================] - 25s 64ms/step - loss: 0.8289 - acc: 0.7106 - val_loss: 0.8433 - val_acc: 0.7094\n",
      "Epoch 8/100\n",
      "391/390 [==============================] - 25s 63ms/step - loss: 0.7901 - acc: 0.7251 - val_loss: 1.0701 - val_acc: 0.6676\n",
      "Epoch 9/100\n",
      "391/390 [==============================] - 25s 63ms/step - loss: 0.7561 - acc: 0.7378 - val_loss: 0.9010 - val_acc: 0.6905\n",
      "Epoch 10/100\n",
      "391/390 [==============================] - 25s 64ms/step - loss: 0.7339 - acc: 0.7426 - val_loss: 0.7360 - val_acc: 0.7445\n",
      "Epoch 11/100\n",
      "391/390 [==============================] - 25s 63ms/step - loss: 0.7107 - acc: 0.7530 - val_loss: 0.7098 - val_acc: 0.7527\n",
      "Epoch 12/100\n",
      "391/390 [==============================] - 24s 62ms/step - loss: 0.6874 - acc: 0.7599 - val_loss: 0.8988 - val_acc: 0.7103\n",
      "Epoch 13/100\n",
      "391/390 [==============================] - 25s 63ms/step - loss: 0.6727 - acc: 0.7658 - val_loss: 0.7923 - val_acc: 0.7286\n",
      "Epoch 14/100\n",
      "391/390 [==============================] - 24s 62ms/step - loss: 0.6542 - acc: 0.7741 - val_loss: 0.7455 - val_acc: 0.7440\n",
      "Epoch 15/100\n",
      "391/390 [==============================] - 24s 62ms/step - loss: 0.6314 - acc: 0.7806 - val_loss: 0.6432 - val_acc: 0.7761\n",
      "Epoch 16/100\n",
      "391/390 [==============================] - 24s 62ms/step - loss: 0.6212 - acc: 0.7852 - val_loss: 0.9455 - val_acc: 0.7036\n",
      "Epoch 17/100\n",
      "391/390 [==============================] - 24s 63ms/step - loss: 0.6078 - acc: 0.7892 - val_loss: 0.7420 - val_acc: 0.7514\n",
      "Epoch 18/100\n",
      "391/390 [==============================] - 24s 62ms/step - loss: 0.5950 - acc: 0.7922 - val_loss: 0.7421 - val_acc: 0.7558\n",
      "Epoch 19/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.5839 - acc: 0.7980 - val_loss: 0.6554 - val_acc: 0.7763\n",
      "Epoch 20/100\n",
      "391/390 [==============================] - 24s 62ms/step - loss: 0.5759 - acc: 0.8009 - val_loss: 0.6579 - val_acc: 0.7769\n",
      "Epoch 21/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.5566 - acc: 0.8080 - val_loss: 0.6880 - val_acc: 0.7698\n",
      "Epoch 22/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.5509 - acc: 0.8076 - val_loss: 0.6079 - val_acc: 0.7950\n",
      "Epoch 23/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.5418 - acc: 0.8133 - val_loss: 0.9362 - val_acc: 0.7233\n",
      "Epoch 24/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.5366 - acc: 0.8149 - val_loss: 0.8773 - val_acc: 0.7266\n",
      "Epoch 25/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.5328 - acc: 0.8163 - val_loss: 0.6102 - val_acc: 0.7903\n",
      "Epoch 26/100\n",
      "391/390 [==============================] - 24s 62ms/step - loss: 0.5127 - acc: 0.8224 - val_loss: 0.6105 - val_acc: 0.7968\n",
      "Epoch 27/100\n",
      "391/390 [==============================] - 24s 62ms/step - loss: 0.5103 - acc: 0.8246 - val_loss: 0.7062 - val_acc: 0.7759\n",
      "Epoch 28/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.5026 - acc: 0.8264 - val_loss: 0.5104 - val_acc: 0.8276\n",
      "Epoch 29/100\n",
      "391/390 [==============================] - 24s 62ms/step - loss: 0.4929 - acc: 0.8299 - val_loss: 0.5164 - val_acc: 0.8277\n",
      "Epoch 30/100\n",
      "391/390 [==============================] - 24s 62ms/step - loss: 0.4927 - acc: 0.8285 - val_loss: 0.5458 - val_acc: 0.8192\n",
      "Epoch 31/100\n",
      "391/390 [==============================] - 24s 62ms/step - loss: 0.4785 - acc: 0.8342 - val_loss: 0.5628 - val_acc: 0.8119\n",
      "Epoch 32/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.4783 - acc: 0.8340 - val_loss: 0.6863 - val_acc: 0.7837\n",
      "Epoch 33/100\n",
      "391/390 [==============================] - 24s 62ms/step - loss: 0.4654 - acc: 0.8386 - val_loss: 0.5506 - val_acc: 0.8129\n",
      "Epoch 34/100\n",
      "391/390 [==============================] - 24s 62ms/step - loss: 0.4674 - acc: 0.8373 - val_loss: 0.6180 - val_acc: 0.7990\n",
      "Epoch 35/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.4552 - acc: 0.8412 - val_loss: 0.5638 - val_acc: 0.8128\n",
      "Epoch 36/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.4555 - acc: 0.8426 - val_loss: 0.5308 - val_acc: 0.8281\n",
      "Epoch 37/100\n",
      "391/390 [==============================] - 24s 62ms/step - loss: 0.4453 - acc: 0.8453 - val_loss: 0.5504 - val_acc: 0.8202\n",
      "Epoch 38/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.4441 - acc: 0.8457 - val_loss: 0.6528 - val_acc: 0.7908\n",
      "Epoch 39/100\n",
      "391/390 [==============================] - 25s 63ms/step - loss: 0.4420 - acc: 0.8466 - val_loss: 0.5208 - val_acc: 0.8307\n",
      "Epoch 40/100\n",
      "391/390 [==============================] - 24s 62ms/step - loss: 0.4344 - acc: 0.8486 - val_loss: 0.6691 - val_acc: 0.7943\n",
      "Epoch 41/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.4335 - acc: 0.8488 - val_loss: 0.5980 - val_acc: 0.8099\n",
      "Epoch 42/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.4237 - acc: 0.8533 - val_loss: 0.5231 - val_acc: 0.8255\n",
      "Epoch 43/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.4214 - acc: 0.8538 - val_loss: 0.5376 - val_acc: 0.8293\n",
      "Epoch 44/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.4153 - acc: 0.8535 - val_loss: 0.5052 - val_acc: 0.8348\n",
      "Epoch 45/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.4201 - acc: 0.8556 - val_loss: 0.6053 - val_acc: 0.8084\n",
      "Epoch 46/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.4121 - acc: 0.8549 - val_loss: 0.5255 - val_acc: 0.8255\n",
      "Epoch 47/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.4087 - acc: 0.8605 - val_loss: 0.4883 - val_acc: 0.8373\n",
      "Epoch 48/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.4026 - acc: 0.8590 - val_loss: 0.5643 - val_acc: 0.8226\n",
      "Epoch 49/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.4033 - acc: 0.8599 - val_loss: 0.5117 - val_acc: 0.8331\n",
      "Epoch 50/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.4011 - acc: 0.8618 - val_loss: 0.4709 - val_acc: 0.8463\n",
      "Epoch 51/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3968 - acc: 0.8619 - val_loss: 0.5713 - val_acc: 0.8178\n",
      "Epoch 52/100\n",
      "391/390 [==============================] - 24s 62ms/step - loss: 0.3937 - acc: 0.8627 - val_loss: 0.4743 - val_acc: 0.8481\n",
      "Epoch 53/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3868 - acc: 0.8653 - val_loss: 0.5812 - val_acc: 0.8214\n",
      "Epoch 54/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3871 - acc: 0.8649 - val_loss: 0.5719 - val_acc: 0.8207\n",
      "Epoch 55/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3854 - acc: 0.8660 - val_loss: 0.5848 - val_acc: 0.8153\n",
      "Epoch 56/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3831 - acc: 0.8651 - val_loss: 0.4901 - val_acc: 0.8355\n",
      "Epoch 57/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3780 - acc: 0.8686 - val_loss: 0.5152 - val_acc: 0.8306\n",
      "Epoch 58/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3793 - acc: 0.8671 - val_loss: 0.5180 - val_acc: 0.8347\n",
      "Epoch 59/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3811 - acc: 0.8684 - val_loss: 0.5407 - val_acc: 0.8247\n",
      "Epoch 60/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3727 - acc: 0.8706 - val_loss: 0.5181 - val_acc: 0.8378\n",
      "Epoch 61/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3710 - acc: 0.8704 - val_loss: 0.5560 - val_acc: 0.8222\n",
      "Epoch 62/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3729 - acc: 0.8704 - val_loss: 0.5582 - val_acc: 0.8283\n",
      "Epoch 63/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3662 - acc: 0.8734 - val_loss: 0.5122 - val_acc: 0.8344\n",
      "Epoch 64/100\n",
      "391/390 [==============================] - 24s 62ms/step - loss: 0.3671 - acc: 0.8710 - val_loss: 0.5020 - val_acc: 0.8393\n",
      "Epoch 65/100\n",
      "391/390 [==============================] - 24s 62ms/step - loss: 0.3544 - acc: 0.8753 - val_loss: 0.6315 - val_acc: 0.8109\n",
      "Epoch 66/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3583 - acc: 0.8756 - val_loss: 0.5983 - val_acc: 0.8175\n",
      "Epoch 67/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3579 - acc: 0.8765 - val_loss: 0.5653 - val_acc: 0.8232\n",
      "Epoch 68/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3554 - acc: 0.8756 - val_loss: 0.5231 - val_acc: 0.8342\n",
      "Epoch 69/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3537 - acc: 0.8777 - val_loss: 0.5350 - val_acc: 0.8303\n",
      "Epoch 70/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3532 - acc: 0.8762 - val_loss: 0.5505 - val_acc: 0.8259\n",
      "Epoch 71/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3493 - acc: 0.8780 - val_loss: 0.5330 - val_acc: 0.8357\n",
      "Epoch 72/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3497 - acc: 0.8774 - val_loss: 0.5406 - val_acc: 0.8296\n",
      "Epoch 73/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3443 - acc: 0.8802 - val_loss: 0.6036 - val_acc: 0.8149\n",
      "Epoch 74/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3364 - acc: 0.8824 - val_loss: 0.6413 - val_acc: 0.8051\n",
      "Epoch 75/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3471 - acc: 0.8803 - val_loss: 0.6051 - val_acc: 0.8162\n",
      "Epoch 76/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3432 - acc: 0.8797 - val_loss: 0.5413 - val_acc: 0.8319\n",
      "Epoch 77/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3320 - acc: 0.8835 - val_loss: 0.5318 - val_acc: 0.8396\n",
      "Epoch 78/100\n",
      "391/390 [==============================] - 24s 62ms/step - loss: 0.3383 - acc: 0.8838 - val_loss: 0.5494 - val_acc: 0.8286\n",
      "Epoch 79/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3361 - acc: 0.8836 - val_loss: 0.5579 - val_acc: 0.8275\n",
      "Epoch 80/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3327 - acc: 0.8838 - val_loss: 0.6010 - val_acc: 0.8170\n",
      "Epoch 81/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3327 - acc: 0.8840 - val_loss: 0.6140 - val_acc: 0.8133\n",
      "Epoch 82/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3292 - acc: 0.8845 - val_loss: 0.5225 - val_acc: 0.8365\n",
      "Epoch 83/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3321 - acc: 0.8852 - val_loss: 0.5947 - val_acc: 0.8192\n",
      "Epoch 84/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3224 - acc: 0.8858 - val_loss: 0.5660 - val_acc: 0.8287\n",
      "Epoch 85/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3307 - acc: 0.8848 - val_loss: 0.4988 - val_acc: 0.8414\n",
      "Epoch 86/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3204 - acc: 0.8888 - val_loss: 0.4920 - val_acc: 0.8439\n",
      "Epoch 87/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3243 - acc: 0.8886 - val_loss: 0.7718 - val_acc: 0.7830\n",
      "Epoch 88/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3160 - acc: 0.8884 - val_loss: 0.6034 - val_acc: 0.8205\n",
      "Epoch 89/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3151 - acc: 0.8893 - val_loss: 0.6789 - val_acc: 0.8064\n",
      "Epoch 90/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3175 - acc: 0.8896 - val_loss: 0.5381 - val_acc: 0.8376\n",
      "Epoch 91/100\n",
      "391/390 [==============================] - 24s 62ms/step - loss: 0.3159 - acc: 0.8906 - val_loss: 0.5788 - val_acc: 0.8227\n",
      "Epoch 92/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3157 - acc: 0.8891 - val_loss: 0.5559 - val_acc: 0.8303\n",
      "Epoch 93/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3135 - acc: 0.8904 - val_loss: 0.6096 - val_acc: 0.8157\n",
      "Epoch 94/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3099 - acc: 0.8912 - val_loss: 0.5398 - val_acc: 0.8344\n",
      "Epoch 95/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3144 - acc: 0.8910 - val_loss: 0.6708 - val_acc: 0.8054\n",
      "Epoch 96/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3102 - acc: 0.8913 - val_loss: 0.5805 - val_acc: 0.8248\n",
      "Epoch 97/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3094 - acc: 0.8919 - val_loss: 0.4815 - val_acc: 0.8458\n",
      "Epoch 98/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3099 - acc: 0.8915 - val_loss: 0.5750 - val_acc: 0.8280\n",
      "Epoch 99/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3090 - acc: 0.8927 - val_loss: 0.4618 - val_acc: 0.8538\n",
      "Epoch 100/100\n",
      "391/390 [==============================] - 24s 61ms/step - loss: 0.3036 - acc: 0.8947 - val_loss: 0.6221 - val_acc: 0.8150\n"
     ]
    }
   ],
   "source": [
    "history = cnn_model.fit_generator(dataGenerator.flow(train_x, train_y, batch_size), epochs = 100, shuffle = True, validation_data = (test_x, test_y), steps_per_epoch=train_x.shape[0] / batch_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "colab_type": "code",
    "id": "e9E2LMiBVZNG",
    "outputId": "4397e573-b3d6-4713-982f-f966e059abf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 146us/step\n",
      "cnn loss : 0.6221367831468582\n",
      "cnn acc : 0.815\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "score = cnn_model.evaluate(test_x,test_y)\n",
    "print(f'cnn loss : {score[0]}')\n",
    "print(f'cnn acc : {score[1]}')\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['acc'], '-', label = 'train_acc', color = 'g')\n",
    "plt.plot(history.history['val_acc'],'--', label = 'valid_acc', color = 'r')\n",
    "plt.title('accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'], '-', label = 'train_loss', color = 'g')\n",
    "plt.plot(history.history['val_loss'],'--', label = 'valid_loss', color = 'r')\n",
    "plt.title('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r_g_70nRJBb0"
   },
   "source": [
    "# 同樣運算 10 個 epochs，但 CNN 在 test data 的準確率顯著優於 DNN!\n",
    "# 作業\n",
    "1.請試著調整各個超參數，並說明那些超參數對於結果有明顯的影響? <br />\n",
    "2.CNN 與 DNN 哪個模型的參數數量比較多? 造成參數的數量不同的原因在哪<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FQxxTmClJWxb"
   },
   "source": [
    "1. BatchSize : small batch size might will have much stable training process and it will have large chane to convergence. But the minimal you found might would have large chane that it is local minimal. In contrast, big batch size will make the training unstable, since your weight was updated everytime after a large amount of data. So we need to find a good batch size for our training which wont make our training drop into local minimal too esay and the weight updating will be much stabler.\n",
    "\n",
    "2. Learning rate : Large learning rate might make our training have hard time to converge, small learning rate might need to spend lots of time on training, and it will easy to drop into local minimal.\n",
    "\n",
    "3. filter number in Conv2D: More filters means more weights(parameters) in our NN, it might make the training time longer and the model more complex. But this means our model can learn more detail of our data, but the generalization ability will be worse.\n",
    "\n",
    "4. Kernel size in Conv2D : Bigger kernel size means our model is learning on larger feature in image, which might make us lost some detail. But the generalization ability will become better.\n",
    "\n",
    "5. Number of hidden layers: At the first few times of training, it might be good idea we can add the hidden as much as we can until the testing loss or testing accuracy is not improving. But the training effort(time) will be big.\n",
    "\n",
    "6. Dropout : Normally, 0.5 will be a good start to setting our dropout rate. Dropout can make our model has better generalizatiob ability. But too big dropout rate might resut in underfitting, since most of the parameters didnt have much chance to be updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-2ex-zuJXqkj"
   },
   "source": [
    "DNN will have much more parameters, since every pixel in the the image will have one weight in one neuron. If our image size is 32*32*3 and the dense unit is 512. Then the total parameters would be 1572864(weights) +  512(bias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iUADKo7KdbbA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Day_097_HW",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
